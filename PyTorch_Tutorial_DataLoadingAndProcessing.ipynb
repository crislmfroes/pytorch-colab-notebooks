{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-Tutorial-DataLoadingAndProcessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crislmfroes/pytorch-colab-notebooks/blob/master/PyTorch_Tutorial_DataLoadingAndProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "g1xOSc5Tm7OV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "ea47fcf1-7789-431f-a1b1-89df9b4d5451"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U torch pandas scikit-image matplotlib torchvision pillow==4.1.1 image"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.23.4)\n",
            "Requirement already up-to-date: scikit-image in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Collecting pillow==4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/e5/88b3d60924a3f8476fa74ec086f5fbaba56dd6cee0d82845f883b6b6dd18/Pillow-4.1.1-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.7MB 4.6MB/s \n",
            "\u001b[?25hRequirement already up-to-date: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow==4.1.1) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.4)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.2)\n",
            "\u001b[31mscikit-image 0.14.1 has requirement pillow>=4.3.0, but you'll have pillow 4.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.3.0\n",
            "    Uninstalling Pillow-4.3.0:\n",
            "      Successfully uninstalled Pillow-4.3.0\n",
            "Successfully installed pillow-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_XYgg87HmYFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.ion()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUGOWhi1pvcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fb3a45f8-b7d2-4fa2-f58d-71d5bb8f4285"
      },
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/faces.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-11 00:25:08--  https://download.pytorch.org/tutorial/faces.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.35.125.120, 13.35.125.93, 13.35.125.80, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.35.125.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5780252 (5.5M) [application/zip]\n",
            "Saving to: ‘faces.zip’\n",
            "\n",
            "faces.zip           100%[===================>]   5.51M  7.60MB/s    in 0.7s    \n",
            "\n",
            "2018-12-11 00:25:09 (7.60 MB/s) - ‘faces.zip’ saved [5780252/5780252]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P9_sEadLp2EL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "0436344f-3dbb-489c-8667-b1ec5505e120"
      },
      "cell_type": "code",
      "source": [
        "!ls data/faces"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0805personali01.jpg\t   3298715079_5af7c78fcb.jpg\n",
            "1084239450_e76e00b7e7.jpg  3325611505_ddc7beffa1.jpg\n",
            "10comm-decarlo.jpg\t   3362762930_24f76cb89c.jpg\n",
            "110276240_bec305da91.jpg   343583208_e986824d77.jpg\n",
            "1198_0_861.jpg\t\t   3461016494_56cce9c984.jpg\n",
            "137341995_e7c48e9a75.jpg   348272697_832ce65324.jpg\n",
            "1383023626_8a49e4879a.jpg  3534188114_2108895291.jpg\n",
            "144044282_87cf3ff76e.jpg   3534189272_8ef88ba368.jpg\n",
            "152601997_ec6429a43c.jpg   3555944509_7b477069c6.jpg\n",
            "1549040388_b99e9fa295.jpg  3574737496_6ee8207045.jpg\n",
            "1878519279_f905d4f34e.jpg  362167809_d5a5dcbfdb.jpg\n",
            "2046713398_91aaa6fe1c.jpg  363149951_8be04dc6c0.jpg\n",
            "2173711035_dbd53b4f9f.jpg  3638950581_3387685d3a.jpg\n",
            "2210514040_6b03ff2629.jpg  3646828311_bfeb429ef7.jpg\n",
            "2322901504_08122b01ba.jpg  3689162471_5f9ffb5aa0.jpg\n",
            "2327253037_66a61ea6fe.jpg  3718903026_c1bf5dfcf8.jpg\n",
            "2328398005_d328a70b4c.jpg  3790616528_297c0ac935.jpg\n",
            "2370961440_6bc8ce346c.jpg  3855944735_e252959937.jpg\n",
            "2382SJ8.jpg\t\t   3856149136_d4595ffdd4.jpg\n",
            "252418361_440b75751b.jpg   3872768751_e60d7fdbd5.jpg\n",
            "262007783_943bbcf613.jpg   529447797_0f9d2fb756.jpg\n",
            "2633371780_45b740b670.jpg  57635685_d41c98f8ca.jpg\n",
            "2647088981_60e9fe40cd.jpg  809285949_6889026b53.jpg\n",
            "2711409561_a0786a3d3d.jpg  92053278_be61a225d2.jpg\n",
            "2722779845_7fcb64a096.jpg  96063776_bdb3617b64.jpg\n",
            "2795838930_0cc5aa5f41.jpg  97308305_4b737d0873.jpg\n",
            "2902323565_100017b63c.jpg  britney-bald.jpg\n",
            "2902760364_89c50bde40.jpg  create_landmark_dataset.py\n",
            "2956581526_cd803f2daa.jpg  deeny.peggy.jpg\n",
            "297448785_b2dda4b2c0.jpg   face_landmarks.csv\n",
            "299733036_fff5ea6f8e.jpg   matt-mathes.jpg\n",
            "303808204_1f744bc407.jpg   person-7.jpg\n",
            "3074791551_baee7fa0c1.jpg  personalpic.jpg\n",
            "3152653555_68322314f3.jpg  person.jpg\n",
            "3264867945_fe18d442c1.jpg  person_TjahjonoDGondhowiardjo.jpg\n",
            "3273658251_b95f65c244.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iHfuxz8Bqboi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8ce9744c-b620-4b99-c98e-ea4e7a7ec334"
      },
      "cell_type": "code",
      "source": [
        "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n",
        "n = 65\n",
        "img_name = landmarks_frame.iloc[n, 0]\n",
        "landmarks = landmarks_frame.iloc[n, 1:].as_matrix()\n",
        "landmarks = landmarks.astype('float').reshape(-1,2)\n",
        "\n",
        "print('Image name: {}'.format(img_name))\n",
        "print('Landmarks shape: {}'.format(landmarks.shape))\n",
        "print('First 4 landmarks: {}'.format(landmarks[:4]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image name: person-7.jpg\n",
            "Landmarks shape: (68, 2)\n",
            "First 4 landmarks: [[32. 65.]\n",
            " [33. 76.]\n",
            " [34. 86.]\n",
            " [34. 97.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8F8XExdTr25t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FaceLandmarksDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.landmarks_frame = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.landmarks_frame)\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
        "    img = io.imread(img_name)\n",
        "    landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix().astype('float').reshape(-1,2)\n",
        "    sample = {'image': img, 'landmarks': landmarks}\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePsIrTJcqECb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mv faces data/faces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aguz8_ndp7Ia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1258
        },
        "outputId": "d9411e10-585a-45fb-c72c-3928f652fa57"
      },
      "cell_type": "code",
      "source": [
        "!unzip faces.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  faces.zip\n",
            "   creating: faces/\n",
            "  inflating: faces/0805personali01.jpg  \n",
            "  inflating: faces/1084239450_e76e00b7e7.jpg  \n",
            "  inflating: faces/10comm-decarlo.jpg  \n",
            "  inflating: faces/110276240_bec305da91.jpg  \n",
            "  inflating: faces/1198_0_861.jpg    \n",
            "  inflating: faces/137341995_e7c48e9a75.jpg  \n",
            "  inflating: faces/1383023626_8a49e4879a.jpg  \n",
            "  inflating: faces/144044282_87cf3ff76e.jpg  \n",
            "  inflating: faces/152601997_ec6429a43c.jpg  \n",
            "  inflating: faces/1549040388_b99e9fa295.jpg  \n",
            "  inflating: faces/1878519279_f905d4f34e.jpg  \n",
            "  inflating: faces/2046713398_91aaa6fe1c.jpg  \n",
            "  inflating: faces/2173711035_dbd53b4f9f.jpg  \n",
            "  inflating: faces/2210514040_6b03ff2629.jpg  \n",
            "  inflating: faces/2322901504_08122b01ba.jpg  \n",
            "  inflating: faces/2327253037_66a61ea6fe.jpg  \n",
            "  inflating: faces/2328398005_d328a70b4c.jpg  \n",
            "  inflating: faces/2370961440_6bc8ce346c.jpg  \n",
            "  inflating: faces/2382SJ8.jpg       \n",
            "  inflating: faces/252418361_440b75751b.jpg  \n",
            "  inflating: faces/262007783_943bbcf613.jpg  \n",
            "  inflating: faces/2633371780_45b740b670.jpg  \n",
            "  inflating: faces/2647088981_60e9fe40cd.jpg  \n",
            "  inflating: faces/2711409561_a0786a3d3d.jpg  \n",
            "  inflating: faces/2722779845_7fcb64a096.jpg  \n",
            "  inflating: faces/2795838930_0cc5aa5f41.jpg  \n",
            "  inflating: faces/2902323565_100017b63c.jpg  \n",
            "  inflating: faces/2902760364_89c50bde40.jpg  \n",
            "  inflating: faces/2956581526_cd803f2daa.jpg  \n",
            "  inflating: faces/297448785_b2dda4b2c0.jpg  \n",
            "  inflating: faces/299733036_fff5ea6f8e.jpg  \n",
            "  inflating: faces/303808204_1f744bc407.jpg  \n",
            "  inflating: faces/3074791551_baee7fa0c1.jpg  \n",
            "  inflating: faces/3152653555_68322314f3.jpg  \n",
            "  inflating: faces/3264867945_fe18d442c1.jpg  \n",
            "  inflating: faces/3273658251_b95f65c244.jpg  \n",
            "  inflating: faces/3298715079_5af7c78fcb.jpg  \n",
            "  inflating: faces/3325611505_ddc7beffa1.jpg  \n",
            "  inflating: faces/3362762930_24f76cb89c.jpg  \n",
            "  inflating: faces/343583208_e986824d77.jpg  \n",
            "  inflating: faces/3461016494_56cce9c984.jpg  \n",
            "  inflating: faces/348272697_832ce65324.jpg  \n",
            "  inflating: faces/3534188114_2108895291.jpg  \n",
            "  inflating: faces/3534189272_8ef88ba368.jpg  \n",
            "  inflating: faces/3555944509_7b477069c6.jpg  \n",
            "  inflating: faces/3574737496_6ee8207045.jpg  \n",
            "  inflating: faces/362167809_d5a5dcbfdb.jpg  \n",
            "  inflating: faces/363149951_8be04dc6c0.jpg  \n",
            "  inflating: faces/3638950581_3387685d3a.jpg  \n",
            "  inflating: faces/3646828311_bfeb429ef7.jpg  \n",
            "  inflating: faces/3689162471_5f9ffb5aa0.jpg  \n",
            "  inflating: faces/3718903026_c1bf5dfcf8.jpg  \n",
            "  inflating: faces/3790616528_297c0ac935.jpg  \n",
            "  inflating: faces/3855944735_e252959937.jpg  \n",
            "  inflating: faces/3856149136_d4595ffdd4.jpg  \n",
            "  inflating: faces/3872768751_e60d7fdbd5.jpg  \n",
            "  inflating: faces/529447797_0f9d2fb756.jpg  \n",
            "  inflating: faces/57635685_d41c98f8ca.jpg  \n",
            "  inflating: faces/809285949_6889026b53.jpg  \n",
            "  inflating: faces/92053278_be61a225d2.jpg  \n",
            "  inflating: faces/96063776_bdb3617b64.jpg  \n",
            "  inflating: faces/97308305_4b737d0873.jpg  \n",
            "  inflating: faces/britney-bald.jpg  \n",
            "  inflating: faces/create_landmark_dataset.py  \n",
            "  inflating: faces/deeny.peggy.jpg   \n",
            "  inflating: faces/face_landmarks.csv  \n",
            "  inflating: faces/matt-mathes.jpg   \n",
            "  inflating: faces/person-7.jpg      \n",
            "  inflating: faces/person.jpg        \n",
            "  inflating: faces/person_TjahjonoDGondhowiardjo.jpg  \n",
            "  inflating: faces/personalpic.jpg   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sB5kZ1sst8bR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        },
        "outputId": "f505f21c-9caf-4cd7-d67c-10ecca23979f"
      },
      "cell_type": "code",
      "source": [
        "face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/')\n",
        "for i in range(len(face_dataset)):\n",
        "  sample = face_dataset[i]\n",
        "  print(i, sample['image'].shape, sample['landmarks'].shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 (324, 215, 3) (68, 2)\n",
            "1 (500, 333, 3) (68, 2)\n",
            "2 (250, 258, 3) (68, 2)\n",
            "3 (434, 290, 3) (68, 2)\n",
            "4 (828, 630, 3) (68, 2)\n",
            "5 (402, 500, 3) (68, 2)\n",
            "6 (332, 500, 3) (68, 2)\n",
            "7 (333, 500, 3) (68, 2)\n",
            "8 (334, 500, 3) (68, 2)\n",
            "9 (376, 500, 3) (68, 2)\n",
            "10 (333, 500, 3) (68, 2)\n",
            "11 (500, 353, 3) (68, 2)\n",
            "12 (500, 335, 3) (68, 2)\n",
            "13 (500, 375, 3) (68, 2)\n",
            "14 (375, 500, 3) (68, 2)\n",
            "15 (500, 375, 3) (68, 2)\n",
            "16 (500, 334, 3) (68, 2)\n",
            "17 (500, 333, 3) (68, 2)\n",
            "18 (303, 200, 3) (68, 2)\n",
            "19 (333, 500, 3) (68, 2)\n",
            "20 (500, 375, 3) (68, 2)\n",
            "21 (333, 500, 3) (68, 2)\n",
            "22 (500, 365, 3) (68, 2)\n",
            "23 (375, 500, 3) (68, 2)\n",
            "24 (333, 500, 3) (68, 2)\n",
            "25 (333, 500, 3) (68, 2)\n",
            "26 (500, 333, 3) (68, 2)\n",
            "27 (500, 342, 3) (68, 2)\n",
            "28 (500, 333, 3) (68, 2)\n",
            "29 (500, 375, 3) (68, 2)\n",
            "30 (500, 334, 3) (68, 2)\n",
            "31 (332, 500, 3) (68, 2)\n",
            "32 (333, 500, 3) (68, 2)\n",
            "33 (500, 333, 3) (68, 2)\n",
            "34 (332, 500, 3) (68, 2)\n",
            "35 (319, 480, 3) (68, 2)\n",
            "36 (333, 500, 3) (68, 2)\n",
            "37 (500, 335, 3) (68, 2)\n",
            "38 (333, 500, 3) (68, 2)\n",
            "39 (500, 375, 3) (68, 2)\n",
            "40 (334, 500, 3) (68, 2)\n",
            "41 (500, 375, 3) (68, 2)\n",
            "42 (333, 500, 3) (68, 2)\n",
            "43 (333, 500, 3) (68, 2)\n",
            "44 (333, 500, 3) (68, 2)\n",
            "45 (333, 500, 3) (68, 2)\n",
            "46 (374, 500, 3) (68, 2)\n",
            "47 (500, 326, 3) (68, 2)\n",
            "48 (333, 500, 3) (68, 2)\n",
            "49 (500, 333, 3) (68, 2)\n",
            "50 (500, 500, 3) (68, 2)\n",
            "51 (500, 334, 3) (68, 2)\n",
            "52 (500, 348, 3) (68, 2)\n",
            "53 (500, 333, 3) (68, 2)\n",
            "54 (500, 333, 3) (68, 2)\n",
            "55 (333, 500, 3) (68, 2)\n",
            "56 (446, 500, 3) (68, 2)\n",
            "57 (306, 500, 3) (68, 2)\n",
            "58 (400, 500, 3) (68, 2)\n",
            "59 (375, 500, 3) (68, 2)\n",
            "60 (375, 500, 3) (68, 2)\n",
            "61 (333, 500, 3) (68, 2)\n",
            "62 (320, 240, 3) (68, 2)\n",
            "63 (195, 163, 3) (68, 2)\n",
            "64 (375, 340, 3) (68, 2)\n",
            "65 (160, 160, 3) (68, 2)\n",
            "66 (239, 209, 3) (68, 2)\n",
            "67 (257, 169, 3) (68, 2)\n",
            "68 (250, 175, 3) (68, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tMhVEb9xSNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Rescale(object):\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    self.output_size = output_size\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    h, w = image.shape[:2]\n",
        "    if isinstance(self.output_size, int):\n",
        "      if h > w:\n",
        "        new_h, new_w = self.output_size * h / w, self.output_size\n",
        "      else:\n",
        "        new_h, new_w = self.output_size, self.output_size * w / h\n",
        "    else:\n",
        "      new_h, new_w = self.output_size\n",
        "    new_h, new_w = int(new_h), int(new_w)\n",
        "    img = transform.resize(image, (new_h, new_w))\n",
        "    landmarks = landmarks * [new_w/w, new_h/h]\n",
        "    return {'image': img, 'landmarks': landmarks}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irvAcDb9zQhN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RandomCrop(object):\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    if isinstance(output_size, int):\n",
        "      self.output_size = (output_size, output_size)\n",
        "    else:\n",
        "      assert len(output_size) == 2\n",
        "      self.output_size = output_size\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    h, w = image.shape[:2]\n",
        "    new_h, new_w = self.output_size\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_h)\n",
        "    image = image[top: top + new_h, left: left + new_w]\n",
        "    landmarks = landmarks - [left, top]\n",
        "    return {'image': image, 'landmarks': landmarks}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVN_5Db71Bse",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    image = image.transpose((2, 0, 1))\n",
        "    return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1wKS-iO16s6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3395aec2-793a-4bdc-a48d-bbb2773cdc8c"
      },
      "cell_type": "code",
      "source": [
        "scale = Rescale(256)\n",
        "crop = RandomCrop(128)\n",
        "composed = transforms.Compose([Rescale(256), RandomCrop(224)])\n",
        "sample = face_dataset[65]\n",
        "for i, trsfrm in enumerate([scale, crop, composed]):\n",
        "  transformed_sample = trsfrm(sample)\n",
        "  print(i, transformed_sample['image'].shape, transformed_sample['landmarks'].shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 (256, 256, 3) (68, 2)\n",
            "1 (128, 128, 3) (68, 2)\n",
            "2 (224, 224, 3) (68, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_NppC6Sf3JS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        },
        "outputId": "f430a4e2-0c93-4488-abc4-197fa4bd2aba"
      },
      "cell_type": "code",
      "source": [
        "transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces',\n",
        "                                          transform=transforms.Compose([\n",
        "                                              Rescale(256),\n",
        "                                              RandomCrop(224),\n",
        "                                              ToTensor()\n",
        "                                          ]))\n",
        "for i in range(len(transformed_dataset)):\n",
        "  sample = transformed_dataset[i]\n",
        "  print(i, sample['image'].size(), sample['landmarks'].size())\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "1 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "2 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "3 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "4 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "5 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "6 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "7 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "8 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "9 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "10 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "11 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "12 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "13 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "14 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "15 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "16 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "17 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "18 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "19 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "20 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "21 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "22 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "23 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "24 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "25 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "26 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "27 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "28 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "29 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "30 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "31 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "32 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "33 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "34 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "35 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "36 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "37 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "38 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "39 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "40 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "41 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "42 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "43 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "44 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "45 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "46 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "47 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "48 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "49 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "50 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "51 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "52 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "53 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "54 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "55 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "56 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "57 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "58 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "59 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "60 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "61 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "62 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "63 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "64 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "65 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "66 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "67 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "68 torch.Size([3, 224, 224]) torch.Size([68, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PpfSQW0z68Cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6842586c-9822-4b29-9b60-6161e0a490e7"
      },
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "  print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "4 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "5 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "6 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "7 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "8 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "9 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "10 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "11 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "12 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "13 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "14 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "15 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "16 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "17 torch.Size([1, 3, 224, 224]) torch.Size([1, 68, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}